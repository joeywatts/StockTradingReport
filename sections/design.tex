
\section{Design}

\subsection{Technology Used}

The virtual portfolio, pricing data, and microblogging sentiment analysis components of this project come together to create a platform for us to test and compare different trading strategies. Each of these components is written in Scala. SBT (Simple Build Tool) is used to organize the project's structure. Each component is made a separate subproject of a stub ``root'' project. SBT helps us manage any dependencies that each subproject may have, including dependencies on other subprojects. We execute our code on a Hadoop/Spark cluster that was provided by Virginia Tech. This cluster also hosts our HBase database. Spark allows us to distribute work loads across a cluster of computers to more efficiently process large datasets and expensive calculations.

The virtual portfolio will maintain a stock portfolio and execute transactions. Specifically, it will store what stocks are currently owned and will facilitate the buying or selling of stocks based on their current stock price (based on the date in the simulated environment). The portfolio will operate within a simulated environment in which we can state the date, so that we can easily backtest with historical data.

\begin{table}
	\centering
	\begin{tabular}{ | r | }
      \hline
      \textsc{stock\_tweets} \\ \hline
      \textbf{post id} (row key) \\ \hline
      base\_data:timestamp \\\hline
      base\_data:text \\ \hline
      base\_data:judgeid \\ \hline
      options:sentiment \\ \hline
      options:symbol \\ \hline
	\end{tabular}
	\begin{tabular}{ | r | }
      \hline
      \textsc{stock\_prices} \\ \hline
      \textbf{symbol\_reverse timestamp} (row key) \\ \hline
      price:price \\ \hline
	\end{tabular}
	\caption{HBase Schemas for stock\_tweets and stock\_prices tables}\label{tab:stocks}
\end{table}

The pricing data API will give access to stock quotes for any given date. For daily stock quotes, we are using the Yahoo Finance API to request quotes for a select set of symbols. We cache the entire set of quotes for a given time interval that we are testing in an HBase table (Table 2.2) for fast access within our cluster. The pricing data is crucial for both evaluating a judge's reliability (assessing the accuracy of their sentiment-based predictions on stock values) and executing trades in the virtual portfolio. The Yahoo Finance API gives us access to daily quotes which allow us to do swing trading, but this data is not granular enough to allow us to execute day trades. To explore day trading algorithms, we will make use of the NYSE's Trade and Quotes database that can be accessed through the Wharton Research Data Services \cite{wrds}. This data set gives intraday stock quotes with millisecond timestamps since 1993. The pricing data HBase table was defined with flexibility in mind, i.e. no assumptions about the granularity of the data were made in the table's schema so that no changes to the schema need to be made to support day trading.

With the virtual portfolio and the pricing data components together, we can define, test, and compare various trading strategies. A trading strategy will be defined with Scala code that interfaces with the virtual portfolio to execute trades (buy or sell stocks) based on pricing information and the analysis of Twitter data including both tweet sentiment and judge reliability. We can assess a strategy's performance by backtesting it with historical stock pricing and tweet data and logging its performance. Then a comparison can be made with other trading strategies.

The code that performs the sentiment analysis of tweets was provided to us by our clients, Saurabh and Eric. Their work on sentiment analysis was done as a term project at Virginia Tech. They have also worked with us to replicate the functions of the CrowdIQ paper to take the reliability of the tweet author into account. Much like the pricing data, our tweet dataset is stored in HBase tables (Table 2.1). We have two tables to store the information relevant to tweets, one for tweets and another for authors (judges). In the tweet table, we store the tweets contents, the judge who authored it, the stock symbols that it pertains to, the analyzed sentiment, and other pertinent information (such as the number of retweets). The judge table stores information about the judge and his/her reliability. This allows us to query HBase for tweets pertaining to certain stock (or those posted in a certain time interval) and aggregate sentiments while factoring in judge reliability.

\subsection{Group Roles}

Apart from Saurabh and Eric, we have a four person team to which we can delegate tasks. Joseph Watts functions as the Data Processing Specialist, which leads the effort on any interactions with the distributed data processing aspect of this project. This will require proficiency with Spark and HBase. Nick Anderson is our Portfolio Management and Machine Learning specialist. Nick will develop the portfolio management system that was detailed earlier and will assist Joseph Mehr in developing and researching machine learning algorithms. Joseph Mehr will specialize in machine learning and will specifically organize the effort behind implementing a Support Vector Machine (SVM). Connor Asbill is our Data Aggregation Researcher. His role involves researching new ways to best aggregate the data that we obtain (tweet sentiments, judge reliability scores, stock price changes, etc) to make informed trading decisions. On top of all of our roles, each of us is responsible for participating in the literature survey that is ongoing throughout this project.

\subsection{Timeline}

A timeline of milestones that plan out the course of our project can be seen below. Our group plans to meet once a week to delegate tasks and review work and research that has been done independently.

\begin{itemize}
\item Feb 15 - Create Virtual Portfolio \& Pricing API
  \begin{itemize}
  \item The virtual portfolio and pricing API are necessary for every trading method that we plan to implement, since they will be used to both train and evaluate the models.
  \end{itemize}
\item Mar 1 - Replicate Base Paper
  \begin{itemize}
  \item “CrowdIQ: A New Opinion Aggregation Model” details a crowd opinion aggregation model, CrowdIQ, which we intend to treat as a baseline which we can compare our new algorithms to.
  \end{itemize}
\item Mar 15 - Determine First Trading Methods
  \begin{itemize}
    \item An extensive literature review is underway in order for us to determine what has been done, what has worked, and what can be improved.
  \end{itemize}
\item Mar 22 - Implement Trading Method \#1
\item Mar 29 - Implement Trading Method \#2
\item Apr 5 - Implement Trading Method \#3 / Create Presentation
\item Apr 12 - Implement Trading Method \#4
\item Apr 19 - Implement Trading Method \#5 / Contrast Methods
\item Apr 25 - Create Final Report
\end{itemize}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../report"
%%% End:
